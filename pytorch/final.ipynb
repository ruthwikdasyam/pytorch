{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. CIFAR10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "Epoch: 1, Batch: 500, Loss: 2.247912\n",
      "Epoch: 1, Batch: 1000, Loss: 2.079398\n",
      "Epoch: 1, Batch: 1500, Loss: 1.990046\n",
      "Epoch: 1, Batch: 2000, Loss: 1.870743\n",
      "Epoch: 1, Batch: 2500, Loss: 1.787618\n",
      "Epoch: 1, Batch: 3000, Loss: 1.735243\n",
      "Epoch: 1, Batch: 3500, Loss: 1.696775\n",
      "Epoch: 1, Batch: 4000, Loss: 1.669765\n",
      "Epoch: 1, Batch: 4500, Loss: 1.636031\n",
      "Epoch: 1, Batch: 5000, Loss: 1.630080\n",
      "Epoch: 1, Batch: 5500, Loss: 1.595795\n",
      "Epoch: 1, Batch: 6000, Loss: 1.512604\n",
      "Epoch: 1, Batch: 6500, Loss: 1.518107\n",
      "Epoch: 1, Batch: 7000, Loss: 1.532138\n",
      "Epoch: 1, Batch: 7500, Loss: 1.479841\n",
      "Epoch: 1, Batch: 8000, Loss: 1.533455\n",
      "Epoch: 1, Batch: 8500, Loss: 1.453073\n",
      "Epoch: 1, Batch: 9000, Loss: 1.477682\n",
      "Epoch: 1, Batch: 9500, Loss: 1.442765\n",
      "Epoch: 1, Batch: 10000, Loss: 1.453797\n",
      "Epoch: 1, Batch: 10500, Loss: 1.379644\n",
      "Epoch: 1, Batch: 11000, Loss: 1.399510\n",
      "Epoch: 1, Batch: 11500, Loss: 1.388594\n",
      "Epoch: 1, Batch: 12000, Loss: 1.391164\n",
      "Epoch: 1, Batch: 12500, Loss: 1.386177\n",
      "Epoch: 2, Batch: 500, Loss: 1.325791\n",
      "Epoch: 2, Batch: 1000, Loss: 1.309918\n",
      "Epoch: 2, Batch: 1500, Loss: 1.360255\n",
      "Epoch: 2, Batch: 2000, Loss: 1.282206\n",
      "Epoch: 2, Batch: 2500, Loss: 1.298154\n",
      "Epoch: 2, Batch: 3000, Loss: 1.267872\n",
      "Epoch: 2, Batch: 3500, Loss: 1.302742\n",
      "Epoch: 2, Batch: 4000, Loss: 1.286830\n",
      "Epoch: 2, Batch: 4500, Loss: 1.297886\n",
      "Epoch: 2, Batch: 5000, Loss: 1.274180\n",
      "Epoch: 2, Batch: 5500, Loss: 1.246478\n",
      "Epoch: 2, Batch: 6000, Loss: 1.222287\n",
      "Epoch: 2, Batch: 6500, Loss: 1.255221\n",
      "Epoch: 2, Batch: 7000, Loss: 1.255139\n",
      "Epoch: 2, Batch: 7500, Loss: 1.253441\n",
      "Epoch: 2, Batch: 8000, Loss: 1.210816\n",
      "Epoch: 2, Batch: 8500, Loss: 1.226565\n",
      "Epoch: 2, Batch: 9000, Loss: 1.203127\n",
      "Epoch: 2, Batch: 9500, Loss: 1.252127\n",
      "Epoch: 2, Batch: 10000, Loss: 1.223361\n",
      "Epoch: 2, Batch: 10500, Loss: 1.213076\n",
      "Epoch: 2, Batch: 11000, Loss: 1.163188\n",
      "Epoch: 2, Batch: 11500, Loss: 1.191037\n",
      "Epoch: 2, Batch: 12000, Loss: 1.144022\n",
      "Epoch: 2, Batch: 12500, Loss: 1.188025\n",
      "Epoch: 3, Batch: 500, Loss: 1.136942\n",
      "Epoch: 3, Batch: 1000, Loss: 1.157003\n",
      "Epoch: 3, Batch: 1500, Loss: 1.130229\n",
      "Epoch: 3, Batch: 2000, Loss: 1.133513\n",
      "Epoch: 3, Batch: 2500, Loss: 1.129292\n",
      "Epoch: 3, Batch: 3000, Loss: 1.101710\n",
      "Epoch: 3, Batch: 3500, Loss: 1.152411\n",
      "Epoch: 3, Batch: 4000, Loss: 1.126561\n",
      "Epoch: 3, Batch: 4500, Loss: 1.146519\n",
      "Epoch: 3, Batch: 5000, Loss: 1.130780\n",
      "Epoch: 3, Batch: 5500, Loss: 1.147905\n",
      "Epoch: 3, Batch: 6000, Loss: 1.100690\n",
      "Epoch: 3, Batch: 6500, Loss: 1.139332\n",
      "Epoch: 3, Batch: 7000, Loss: 1.080110\n",
      "Epoch: 3, Batch: 7500, Loss: 1.098157\n",
      "Epoch: 3, Batch: 8000, Loss: 1.123308\n",
      "Epoch: 3, Batch: 8500, Loss: 1.076718\n",
      "Epoch: 3, Batch: 9000, Loss: 1.146266\n",
      "Epoch: 3, Batch: 9500, Loss: 1.077723\n",
      "Epoch: 3, Batch: 10000, Loss: 1.096702\n",
      "Epoch: 3, Batch: 10500, Loss: 1.153529\n",
      "Epoch: 3, Batch: 11000, Loss: 1.091375\n",
      "Epoch: 3, Batch: 11500, Loss: 1.114915\n",
      "Epoch: 3, Batch: 12000, Loss: 1.104924\n",
      "Epoch: 3, Batch: 12500, Loss: 1.057760\n",
      "Epoch: 4, Batch: 500, Loss: 1.039978\n",
      "Epoch: 4, Batch: 1000, Loss: 1.047975\n",
      "Epoch: 4, Batch: 1500, Loss: 1.029196\n",
      "Epoch: 4, Batch: 2000, Loss: 1.041863\n",
      "Epoch: 4, Batch: 2500, Loss: 1.060660\n",
      "Epoch: 4, Batch: 3000, Loss: 1.005008\n",
      "Epoch: 4, Batch: 3500, Loss: 1.024479\n",
      "Epoch: 4, Batch: 4000, Loss: 1.035341\n",
      "Epoch: 4, Batch: 4500, Loss: 1.046792\n",
      "Epoch: 4, Batch: 5000, Loss: 1.071396\n",
      "Epoch: 4, Batch: 5500, Loss: 1.067931\n",
      "Epoch: 4, Batch: 6000, Loss: 1.042371\n",
      "Epoch: 4, Batch: 6500, Loss: 1.064549\n",
      "Epoch: 4, Batch: 7000, Loss: 1.050619\n",
      "Epoch: 4, Batch: 7500, Loss: 1.029524\n",
      "Epoch: 4, Batch: 8000, Loss: 1.066560\n",
      "Epoch: 4, Batch: 8500, Loss: 0.977414\n",
      "Epoch: 4, Batch: 9000, Loss: 1.068090\n",
      "Epoch: 4, Batch: 9500, Loss: 1.023277\n",
      "Epoch: 4, Batch: 10000, Loss: 1.035851\n",
      "Epoch: 4, Batch: 10500, Loss: 1.000867\n",
      "Epoch: 4, Batch: 11000, Loss: 1.026768\n",
      "Epoch: 4, Batch: 11500, Loss: 1.046974\n",
      "Epoch: 4, Batch: 12000, Loss: 1.004987\n",
      "Epoch: 4, Batch: 12500, Loss: 1.004489\n",
      "Epoch: 5, Batch: 500, Loss: 0.986270\n",
      "Epoch: 5, Batch: 1000, Loss: 1.010289\n",
      "Epoch: 5, Batch: 1500, Loss: 0.941722\n",
      "Epoch: 5, Batch: 2000, Loss: 0.971597\n",
      "Epoch: 5, Batch: 2500, Loss: 0.983394\n",
      "Epoch: 5, Batch: 3000, Loss: 0.973434\n",
      "Epoch: 5, Batch: 3500, Loss: 0.981358\n",
      "Epoch: 5, Batch: 4000, Loss: 0.986727\n",
      "Epoch: 5, Batch: 4500, Loss: 1.016242\n",
      "Epoch: 5, Batch: 5000, Loss: 1.017138\n",
      "Epoch: 5, Batch: 5500, Loss: 0.985543\n",
      "Epoch: 5, Batch: 6000, Loss: 0.995758\n",
      "Epoch: 5, Batch: 6500, Loss: 0.946267\n",
      "Epoch: 5, Batch: 7000, Loss: 0.968013\n",
      "Epoch: 5, Batch: 7500, Loss: 0.990524\n",
      "Epoch: 5, Batch: 8000, Loss: 0.976469\n",
      "Epoch: 5, Batch: 8500, Loss: 0.937784\n",
      "Epoch: 5, Batch: 9000, Loss: 0.989826\n",
      "Epoch: 5, Batch: 9500, Loss: 0.955573\n",
      "Epoch: 5, Batch: 10000, Loss: 1.010756\n",
      "Epoch: 5, Batch: 10500, Loss: 0.973374\n",
      "Epoch: 5, Batch: 11000, Loss: 0.957093\n",
      "Epoch: 5, Batch: 11500, Loss: 0.975214\n",
      "Epoch: 5, Batch: 12000, Loss: 0.953772\n",
      "Epoch: 5, Batch: 12500, Loss: 0.979621\n",
      "Epoch: 6, Batch: 500, Loss: 0.943574\n",
      "Epoch: 6, Batch: 1000, Loss: 0.923815\n",
      "Epoch: 6, Batch: 1500, Loss: 0.918824\n",
      "Epoch: 6, Batch: 2000, Loss: 0.914730\n",
      "Epoch: 6, Batch: 2500, Loss: 0.965442\n",
      "Epoch: 6, Batch: 3000, Loss: 0.873335\n",
      "Epoch: 6, Batch: 3500, Loss: 0.940695\n",
      "Epoch: 6, Batch: 4000, Loss: 0.934130\n",
      "Epoch: 6, Batch: 4500, Loss: 0.939031\n",
      "Epoch: 6, Batch: 5000, Loss: 0.910272\n",
      "Epoch: 6, Batch: 5500, Loss: 0.942437\n",
      "Epoch: 6, Batch: 6000, Loss: 0.987556\n",
      "Epoch: 6, Batch: 6500, Loss: 0.937229\n",
      "Epoch: 6, Batch: 7000, Loss: 0.914034\n",
      "Epoch: 6, Batch: 7500, Loss: 0.947477\n",
      "Epoch: 6, Batch: 8000, Loss: 0.923945\n",
      "Epoch: 6, Batch: 8500, Loss: 0.956444\n",
      "Epoch: 6, Batch: 9000, Loss: 0.949821\n",
      "Epoch: 6, Batch: 9500, Loss: 0.963619\n",
      "Epoch: 6, Batch: 10000, Loss: 0.954191\n",
      "Epoch: 6, Batch: 10500, Loss: 0.942413\n",
      "Epoch: 6, Batch: 11000, Loss: 0.922976\n",
      "Epoch: 6, Batch: 11500, Loss: 0.935371\n",
      "Epoch: 6, Batch: 12000, Loss: 0.972085\n",
      "Epoch: 6, Batch: 12500, Loss: 0.967457\n",
      "Epoch: 7, Batch: 500, Loss: 0.885973\n",
      "Epoch: 7, Batch: 1000, Loss: 0.912759\n",
      "Epoch: 7, Batch: 1500, Loss: 0.893990\n",
      "Epoch: 7, Batch: 2000, Loss: 0.852700\n",
      "Epoch: 7, Batch: 2500, Loss: 0.858327\n",
      "Epoch: 7, Batch: 3000, Loss: 0.893164\n",
      "Epoch: 7, Batch: 3500, Loss: 0.907861\n",
      "Epoch: 7, Batch: 4000, Loss: 0.917698\n",
      "Epoch: 7, Batch: 4500, Loss: 0.928684\n",
      "Epoch: 7, Batch: 5000, Loss: 0.896969\n",
      "Epoch: 7, Batch: 5500, Loss: 0.932674\n",
      "Epoch: 7, Batch: 6000, Loss: 0.901420\n",
      "Epoch: 7, Batch: 6500, Loss: 0.880277\n",
      "Epoch: 7, Batch: 7000, Loss: 0.917337\n",
      "Epoch: 7, Batch: 7500, Loss: 0.899095\n",
      "Epoch: 7, Batch: 8000, Loss: 0.869540\n",
      "Epoch: 7, Batch: 8500, Loss: 0.914049\n",
      "Epoch: 7, Batch: 9000, Loss: 0.921891\n",
      "Epoch: 7, Batch: 9500, Loss: 0.891198\n",
      "Epoch: 7, Batch: 10000, Loss: 0.863961\n",
      "Epoch: 7, Batch: 10500, Loss: 0.889872\n",
      "Epoch: 7, Batch: 11000, Loss: 0.915873\n",
      "Epoch: 7, Batch: 11500, Loss: 0.927417\n",
      "Epoch: 7, Batch: 12000, Loss: 0.913977\n",
      "Epoch: 7, Batch: 12500, Loss: 0.943126\n",
      "Epoch: 8, Batch: 500, Loss: 0.882505\n",
      "Epoch: 8, Batch: 1000, Loss: 0.812795\n",
      "Epoch: 8, Batch: 1500, Loss: 0.850436\n",
      "Epoch: 8, Batch: 2000, Loss: 0.897769\n",
      "Epoch: 8, Batch: 2500, Loss: 0.846217\n",
      "Epoch: 8, Batch: 3000, Loss: 0.860121\n",
      "Epoch: 8, Batch: 3500, Loss: 0.848727\n",
      "Epoch: 8, Batch: 4000, Loss: 0.841774\n",
      "Epoch: 8, Batch: 4500, Loss: 0.862322\n",
      "Epoch: 8, Batch: 5000, Loss: 0.872124\n",
      "Epoch: 8, Batch: 5500, Loss: 0.862384\n",
      "Epoch: 8, Batch: 6000, Loss: 0.851862\n",
      "Epoch: 8, Batch: 6500, Loss: 0.879567\n",
      "Epoch: 8, Batch: 7000, Loss: 0.847461\n",
      "Epoch: 8, Batch: 7500, Loss: 0.909576\n",
      "Epoch: 8, Batch: 8000, Loss: 0.906170\n",
      "Epoch: 8, Batch: 8500, Loss: 0.883048\n",
      "Epoch: 8, Batch: 9000, Loss: 0.848599\n",
      "Epoch: 8, Batch: 9500, Loss: 0.874481\n",
      "Epoch: 8, Batch: 10000, Loss: 0.866963\n",
      "Epoch: 8, Batch: 10500, Loss: 0.886617\n",
      "Epoch: 8, Batch: 11000, Loss: 0.826063\n",
      "Epoch: 8, Batch: 11500, Loss: 0.923082\n",
      "Epoch: 8, Batch: 12000, Loss: 0.934648\n",
      "Epoch: 8, Batch: 12500, Loss: 0.892352\n",
      "Epoch: 9, Batch: 500, Loss: 0.787857\n",
      "Epoch: 9, Batch: 1000, Loss: 0.828179\n",
      "Epoch: 9, Batch: 1500, Loss: 0.816751\n",
      "Epoch: 9, Batch: 2000, Loss: 0.732267\n",
      "Epoch: 9, Batch: 2500, Loss: 0.791842\n",
      "Epoch: 9, Batch: 3000, Loss: 0.867266\n",
      "Epoch: 9, Batch: 3500, Loss: 0.820181\n",
      "Epoch: 9, Batch: 4000, Loss: 0.832374\n",
      "Epoch: 9, Batch: 4500, Loss: 0.837175\n",
      "Epoch: 9, Batch: 5000, Loss: 0.878206\n",
      "Epoch: 9, Batch: 5500, Loss: 0.847229\n",
      "Epoch: 9, Batch: 6000, Loss: 0.852256\n",
      "Epoch: 9, Batch: 6500, Loss: 0.906872\n",
      "Epoch: 9, Batch: 7000, Loss: 0.860283\n",
      "Epoch: 9, Batch: 7500, Loss: 0.873158\n",
      "Epoch: 9, Batch: 8000, Loss: 0.814898\n",
      "Epoch: 9, Batch: 8500, Loss: 0.838562\n",
      "Epoch: 9, Batch: 9000, Loss: 0.829654\n",
      "Epoch: 9, Batch: 9500, Loss: 0.837196\n",
      "Epoch: 9, Batch: 10000, Loss: 0.870024\n",
      "Epoch: 9, Batch: 10500, Loss: 0.816204\n",
      "Epoch: 9, Batch: 11000, Loss: 0.856277\n",
      "Epoch: 9, Batch: 11500, Loss: 0.856276\n",
      "Epoch: 9, Batch: 12000, Loss: 0.887964\n",
      "Epoch: 9, Batch: 12500, Loss: 0.864042\n",
      "Epoch: 10, Batch: 500, Loss: 0.794586\n",
      "Epoch: 10, Batch: 1000, Loss: 0.778495\n",
      "Epoch: 10, Batch: 1500, Loss: 0.783624\n",
      "Epoch: 10, Batch: 2000, Loss: 0.827420\n",
      "Epoch: 10, Batch: 2500, Loss: 0.802758\n",
      "Epoch: 10, Batch: 3000, Loss: 0.781270\n",
      "Epoch: 10, Batch: 3500, Loss: 0.848113\n",
      "Epoch: 10, Batch: 4000, Loss: 0.786479\n",
      "Epoch: 10, Batch: 4500, Loss: 0.810451\n",
      "Epoch: 10, Batch: 5000, Loss: 0.803286\n",
      "Epoch: 10, Batch: 5500, Loss: 0.818481\n",
      "Epoch: 10, Batch: 6000, Loss: 0.833116\n",
      "Epoch: 10, Batch: 6500, Loss: 0.831749\n",
      "Epoch: 10, Batch: 7000, Loss: 0.875378\n",
      "Epoch: 10, Batch: 7500, Loss: 0.856729\n",
      "Epoch: 10, Batch: 8000, Loss: 0.819159\n",
      "Epoch: 10, Batch: 8500, Loss: 0.859444\n",
      "Epoch: 10, Batch: 9000, Loss: 0.847483\n",
      "Epoch: 10, Batch: 9500, Loss: 0.842599\n",
      "Epoch: 10, Batch: 10000, Loss: 0.797765\n",
      "Epoch: 10, Batch: 10500, Loss: 0.822089\n",
      "Epoch: 10, Batch: 11000, Loss: 0.846862\n",
      "Epoch: 10, Batch: 11500, Loss: 0.863988\n",
      "Epoch: 10, Batch: 12000, Loss: 0.868818\n",
      "Epoch: 10, Batch: 12500, Loss: 0.866415\n",
      "Accuracy of the model on the test images: 71.75%\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define the CNN architecture\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 32, 3, 1)  # Adjusted for 3 input channels, no padding\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)  # No padding\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(64 * 6 * 6, 128)  # Correct calculation of the flattened size\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.max_pool2d(x, 2)  # Reduces size to 15x15\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.max_pool2d(x, 2)  # Reduces size to 6x6\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "\n",
    "# Load and transform data\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=4, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=4, shuffle=False, num_workers=2)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training function\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "            if batch_idx % 500 == 499:\n",
    "                print(f'Epoch: {epoch+1}, Batch: {batch_idx+1}, Loss: {running_loss / 500:.6f}')\n",
    "                running_loss = 0.0\n",
    "\n",
    "# Testing function\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Accuracy of the model on the test images: {accuracy:.2f}%')\n",
    "\n",
    "# Execute training and testing\n",
    "train(model, device, train_loader, optimizer, criterion, num_epochs=10)\n",
    "test(model, device, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ruthwik/anaconda3/envs/transformers/lib/python3.9/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/ruthwik/anaconda3/envs/transformers/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 1.624929\n",
      "Epoch: 2, Loss: 1.057229\n",
      "Epoch: 3, Loss: 0.780996\n",
      "Epoch: 4, Loss: 0.590915\n",
      "Epoch: 5, Loss: 0.433436\n",
      "Epoch: 6, Loss: 0.319630\n",
      "Epoch: 7, Loss: 0.229362\n",
      "Epoch: 8, Loss: 0.178540\n",
      "Epoch: 9, Loss: 0.139547\n",
      "Epoch: 10, Loss: 0.125846\n",
      "Epoch: 11, Loss: 0.105844\n",
      "Epoch: 12, Loss: 0.090893\n",
      "Epoch: 13, Loss: 0.084409\n",
      "Epoch: 14, Loss: 0.082470\n",
      "Epoch: 15, Loss: 0.077677\n",
      "Epoch: 16, Loss: 0.068802\n",
      "Epoch: 17, Loss: 0.069172\n",
      "Epoch: 18, Loss: 0.065412\n",
      "Epoch: 19, Loss: 0.065732\n",
      "Epoch: 20, Loss: 0.058172\n",
      "Epoch: 21, Loss: 0.056878\n",
      "Epoch: 22, Loss: 0.062114\n",
      "Epoch: 23, Loss: 0.056140\n"
     ]
    }
   ],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "model = torchvision.models.vgg11(pretrained=False, num_classes=10).to(device)\n",
    "model.features[0].weight = nn.init.kaiming_normal_(model.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        print(f'Epoch: {epoch+1}, Loss: {running_loss / len(train_loader):.6f}')\n",
    "    return train_losses\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Run training and testing\n",
    "num_epochs = 100\n",
    "train_losses = train(model, device, train_loader, optimizer, criterion, num_epochs)\n",
    "test_accuracy = test(model, device, test_loader)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2.1 b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Model setup with Batch Normalization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "model_bn = torchvision.models.vgg11_bn(pretrained=False, num_classes=10).to(device)  # Switch to vgg11_bn\n",
    "model_bn.features[0].weight = nn.init.kaiming_normal_(model_bn.features[0].weight, mode='fan_out', nonlinearity='relu')\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model_bn.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        print(f'Epoch: {epoch+1}, Loss: {running_loss / len(train_loader):.6f}')\n",
    "    return train_losses\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Run training and testing\n",
    "num_epochs = 100\n",
    "train_losses = train(model_bn, device, train_loader, optimizer, criterion, num_epochs)\n",
    "test_accuracy = test(model_bn, device, test_loader)\n",
    "\n",
    "\n",
    "# Plotting and Saving the Plot\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.1c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Replace ReLU with LeakyReLU in the model\n",
    "def replace_relu_with_leaky(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "        elif isinstance(child, nn.Sequential):\n",
    "            for sub_child_name, sub_child in child.named_children():\n",
    "                if isinstance(sub_child, nn.ReLU):\n",
    "                    setattr(child, sub_child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "                replace_relu_with_leaky(sub_child)\n",
    "\n",
    "# Model setup with LeakyReLU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "model_leaky_relu = torchvision.models.vgg11(pretrained=False, num_classes=10).to(device)\n",
    "replace_relu_with_leaky(model_leaky_relu)\n",
    "model_leaky_relu.features[0].weight = nn.init.kaiming_normal_(model_leaky_relu.features[0].weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = optim.Adam(model_leaky_relu.parameters(), lr=2e-4, betas=(0.5, 0.999))\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        print(f'Epoch: {epoch+1}, Loss: {running_loss / len(train_loader):.6f}')\n",
    "    return train_losses\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Run training and testing\n",
    "num_epochs = 100\n",
    "train_losses = train(model_leaky_relu, device, train_loader, optimizer, criterion, num_epochs)\n",
    "test_accuracy = test(model_leaky_relu, device, test_loader)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2 a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=64, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=64, shuffle=False, num_workers=2)\n",
    "\n",
    "# Replace ReLU with LeakyReLU in the model\n",
    "def replace_relu_with_leaky(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "        elif isinstance(child, nn.Sequential):\n",
    "            for sub_child_name, sub_child in child.named_children():\n",
    "                if isinstance(sub_child, nn.ReLU):\n",
    "                    setattr(child, sub_child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "                replace_relu_with_leaky(sub_child)\n",
    "\n",
    "# Model setup with LeakyReLU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "model_leaky_relu = torchvision.models.vgg11(pretrained=False, num_classes=10).to(device)\n",
    "replace_relu_with_leaky(model_leaky_relu)\n",
    "model_leaky_relu.features[0].weight = nn.init.kaiming_normal_(model_leaky_relu.features[0].weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "\n",
    "# Optimizer and loss - Change from Adam to SGD\n",
    "optimizer = optim.SGD(model_leaky_relu.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        print(f'Epoch: {epoch+1}, Loss: {running_loss / len(train_loader):.6f}')\n",
    "    return train_losses\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Run training and testing\n",
    "num_epochs = 100\n",
    "train_losses = train(model_leaky_relu, device, train_loader, optimizer, criterion, num_epochs)\n",
    "test_accuracy = test(model_leaky_relu, device, test_loader)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data loading with adjusted batch size\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)  # Changed batch size to 256\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)  # Changed batch size to 256\n",
    "\n",
    "# Replace ReLU with LeakyReLU in the model\n",
    "def replace_relu_with_leaky(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "        elif isinstance(child, nn.Sequential):\n",
    "            for sub_child_name, sub_child in child.named_children():\n",
    "                if isinstance(sub_child, nn.ReLU):\n",
    "                    setattr(child, sub_child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "                replace_relu_with_leaky(sub_child)\n",
    "\n",
    "# Model setup with LeakyReLU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "model_leaky_relu = torchvision.models.vgg11(pretrained=False, num_classes=10).to(device)\n",
    "replace_relu_with_leaky(model_leaky_relu)\n",
    "model_leaky_relu.features[0].weight = nn.init.kaiming_normal_(model_leaky_relu.features[0].weight, mode='fan_out', nonlinearity='leaky_relu')\n",
    "\n",
    "# Optimizer and loss - Using SGD as previously set\n",
    "optimizer = optim.SGD(model_leaky_relu.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        print(f'Epoch: {epoch+1}, Loss: {running_loss / len(train_loader):.6f}')\n",
    "    return train_losses\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Run training and testing\n",
    "num_epochs = 100\n",
    "train_losses = train(model_leaky_relu, device, train_loader, optimizer, criterion, num_epochs)\n",
    "test_accuracy = test(model_leaky_relu, device, test_loader)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "# Replace ReLU with LeakyReLU in the model and apply Xavier Uniform initialization\n",
    "def replace_relu_with_leaky_and_init_xavier(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "        elif isinstance(child, nn.Sequential):\n",
    "            for sub_child_name, sub_child in child.named_children():\n",
    "                if isinstance(sub_child, nn.ReLU):\n",
    "                    setattr(child, sub_child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "                replace_relu_with_leaky_and_init_xavier(sub_child)\n",
    "        if isinstance(child, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(child.weight)\n",
    "\n",
    "# Model setup with LeakyReLU and Xavier Uniform initialization\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "model_xavier = torchvision.models.vgg11(pretrained=False, num_classes=10).to(device)\n",
    "replace_relu_with_leaky_and_init_xavier(model_xavier)\n",
    "\n",
    "# Optimizer and loss - Using SGD as previously set\n",
    "optimizer = optim.SGD(model_xavier.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        print(f'Epoch: {epoch+1}, Loss: {running_loss / len(train_loader):.6f}')\n",
    "    return train_losses\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Run training and testing\n",
    "num_epochs = 100\n",
    "train_losses = train(model_xavier, device, train_loader, optimizer, criterion, num_epochs)\n",
    "test_accuracy = test(model_xavier, device, test_loader)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2.2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "# Data loading\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform)\n",
    "train_loader = DataLoader(trainset, batch_size=256, shuffle=True, num_workers=2)\n",
    "\n",
    "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "test_loader = DataLoader(testset, batch_size=256, shuffle=False, num_workers=2)\n",
    "\n",
    "# Replace ReLU with LeakyReLU, apply Xavier Uniform initialization, and disable dropout\n",
    "def modify_model(model):\n",
    "    for child_name, child in model.named_children():\n",
    "        if isinstance(child, nn.ReLU):\n",
    "            setattr(model, child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "        elif isinstance(child, nn.Sequential):\n",
    "            for sub_child_name, sub_child in child.named_children():\n",
    "                if isinstance(sub_child, nn.ReLU):\n",
    "                    setattr(child, sub_child_name, nn.LeakyReLU(negative_slope=0.01))\n",
    "                if isinstance(sub_child, nn.Dropout):\n",
    "                    setattr(child, sub_child_name, nn.Dropout(p=0))  # Disable dropout\n",
    "                modify_model(sub_child)\n",
    "        if isinstance(child, nn.Conv2d):\n",
    "            nn.init.xavier_uniform_(child.weight)\n",
    "\n",
    "# Model setup with modifications\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"CPU\")\n",
    "torch.cuda.memory_summary(device=None, abbreviated=False)\n",
    "model_no_dropout = torchvision.models.vgg11(pretrained=False, num_classes=10).to(device)\n",
    "modify_model(model_no_dropout)\n",
    "\n",
    "# Optimizer and loss - Using SGD as previously set\n",
    "optimizer = optim.SGD(model_no_dropout.parameters(), lr=0.01, momentum=0.9)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Training and testing functions\n",
    "def train(model, device, train_loader, optimizer, criterion, num_epochs):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for batch_idx, (data, targets) in enumerate(train_loader):\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "        train_losses.append(running_loss / len(train_loader))\n",
    "        print(f'Epoch: {epoch+1}, Loss: {running_loss / len(train_loader):.6f}')\n",
    "    return train_losses\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data, targets in test_loader:\n",
    "            data, targets = data.to(device), targets.to(device)\n",
    "            outputs = model(data)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += targets.size(0)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f'Test Accuracy: {accuracy:.2f}%')\n",
    "    return accuracy\n",
    "\n",
    "# Run training and testing\n",
    "num_epochs = 100\n",
    "train_losses = train(model_no_dropout, device, train_loader, optimizer, criterion, num_epochs)\n",
    "test_accuracy = test(model_no_dropout, device, test_loader)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(num_epochs), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss Over Time')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
